---
layout: post
title: Machine Learning Yearning 笔记
categories: [Machine Learning]
description: Machine Learning Yearning 笔记
keywords: Machine Learning
---

吴恩达 ML Yearning 学习笔记

该书很好地结合了作者在工业界的机器学习经验，每篇文章阐述一个经验，篇幅较短，对于机器学习的编程和应用有指导意义。

---

## 1 验证集和测试集

> 验证集和测试集的定义

- 将数据分为`训练集`、`验证集`和`测试集`，验证集和测试集用于检测算法性能。验证集和测试集最好能表示未来的没有见过的样本的情况，这样才能提高模型在实际情况中的泛化性能。

> 验证集和测试集应该服从同一分布

- 验证集和测试集应该服从同一分布。
- 如果一个系统在验证集上运行性能良好，却在测试集上效果不佳的现象，可能的原因有：
    
    1. 训练集和验证集服从同一分布：

        算法在验证集上过拟合了。

    2. 训练集和验证集不服从同一分布：
        
        2.1 算法在验证集上过拟合了。
        
        2.2 测试集比验证集更难预测。

        2.3 测试集和验证集分布不同。

> 验证集和测试集大小

- 验证集的规模应该尽可能地大，应该在 1,000 到 10,000 个样本数据之间。这样在区分不同算法之间的差异的时候，才能区分出来 90% 和 90.1% 的区别。
- 测试集的大小应该大到使你能够对整体系统的性能进行一个高度可信的评估，常用 30% 划分，但是也不是越大越好。

> 使用单值评估指标进行优化

- 单值评估指标（single-number evaluation metric）：如分类准确率，用于评价和优化单个算法。
- 多值评估指标：如查准率（Precision， 精度）和查全率（Recall，召回率）。

    > 猫分类器的查准率指的是在开发集（或测试集）内，那些已经被预测为“猫”的图片之中，实际类别是“猫”的样本比例。而查全率指的是在开发集（或测试集）内，所有实际类别为“猫”的图片中，被正确预测为“猫”的样本比例。人们常常在查准率和查全率之间权衡取舍。
- 组合多个指标评估 如 F1 Score：查准率和查全率的调和平均数，计算公式为 2 / ( (1/Precision) + (1/Recall) )

> 满意度指标 和 优化指标

- 对于一个系统，有 N 项评价标准，可以设置 N - 1 项 `满意度指标`，要求这些指标在一定范围内，然后下一步再根据`优化指标`来优化系统。
  
    > 举例，假设你正在设计一个硬件设备，该设备可以根据用户设置的特殊 “唤醒词” 来唤醒系统，类似于 苹果（Apple） Siri 的监听词为 “Hey Siri”。我们关心的指标是假正例率（false positive rate）—— 用户没有说出唤醒词，系统却被唤醒了，以及假反例率（false negative rate）——用户说出了唤醒词，系统却没能正确被唤醒。这个系统的一个较为合理的优化对象是尝试去最小化假反例率（优化指标），减少用户说出唤醒词而系统却没能正确唤醒的发生率，同时设置约束为每 24 小时不超过一次误报（满意度指标）。

> 验证集 和 度量指标 加速迭代
- idea -> code -> experiment
- 在验证集上评估系统性能就可以帮助你判断当前的方向是否正确。

> 修改验证集、测试集和评价指标
- 对两个分类器根据某指标比较，实验结果和真实部署后出现相反结果的原因：
    
    1. 初始的验证集和测试集与实际的数据的分布不同
    2. 算法在验证集上过拟合了
    3. 该指标不是项目应该优化的指标



## 参考
1. [机器学习训练秘籍](https://accepteddoge.github.io/machine-learning-yearning-cn/)
2. [Machine Learning Yearning](http://www.mlyearning.org/)