---
layout: post
title: EM 算法推导
categories: [Machine Learning]
description: EM 算法
keywords: EM
---

EM 算法是一种无监督的聚类方法，本文对其进行推导

---

# 概述
EM 算法利用极大似然估计法，对于含有隐变量的模型参数进行迭代估计，每次迭代分为两步：E(expectation)步，求期望；M(maximization)步，求极大值。

# 数学基础
先对EM 算法涉及到的数学基础进行回忆，如果已经掌握可以跳过本部分。

## 极大似然估计
极大似然估计是一种概率论中参数估计方法，常见参数估计方法如下：

![mark](http://pcxhsqn8a.bkt.clouddn.com/blog/181115/Fj40ac0BE3.png?imageslim)

极大似然估计的思想是，现在已经获得样本 $x_{1}$, $x_{2}$, ..., $x_{n}$，表明取到这一组样本的概率比较大，将似然函数$L(\theta)$定义为取到这一组变量的概率，那么通过使似然函数最大所得到的$\hat{\theta}$值，就可以作为参数$\theta$的估计值。

$\hat{\theta}(x_{1}$, $x_{2}$, ..., $x_{n})$ 称为参数$\theta$的最大似然估计值，$\theta(x_{1}$, $x_{2}$, ..., $x_{n})$ 称为参数$\theta$的最大似然估计量。

对于离散随机变量有：

$$L(\theta) = L(x_{1}, x_{2}, ..., x_{n};\theta) = \prod_{i=1}^{n}p(x_{i};\theta), \theta\in\Theta$$

对于连续变量：

$$L(\theta) = L(x_{1}, x_{2}, ..., x_{n};\theta) = \prod_{i=1}^{n}f(x_{i};\theta), \theta\in\Theta$$

$$L(x_{1}, x_{2}, ..., x_{n};\hat{\theta}) = max_{\theta\in\Theta} L(x_{1}, x_{2}, ..., x_{n};\theta)$$

因为$L(\theta)$和$\ln{L(\theta)}$在同一个地方取到极值，且求对数对于求解较为方便，因此我们采用对数似然方程：

$$\frac{d}{d\theta}\ln{L(\theta)} = 0$$

若包含$k$个待估参数，则：

$$\frac{\partial}{\partial\theta}\ln{L(\theta_{i})} = 0, i = 1, 2, ...,k$$

联立$k$个方程组即可求解待估参数。
















