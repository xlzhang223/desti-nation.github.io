---
layout: post
title: EM 算法推导
categories: [Machine Learning]
description: EM 算法
keywords: EM
---

EM 算法是一种无监督的聚类方法，本文对其进行推导

---

# 概述
EM 算法利用极大似然估计法，对于含有隐变量的模型参数进行迭代估计，每次迭代分为两步：E(expectation)步，求期望；M(maximization)步，求极大值。

# 数学基础
先对EM 算法涉及到的数学基础进行回忆，如果已经掌握可以跳过本部分。

## 极大似然估计
极大似然估计是一种概率论中参数估计方法，常见参数估计方法如下：

![mark](http://pcxhsqn8a.bkt.clouddn.com/blog/181115/Fj40ac0BE3.png?imageslim)

极大似然估计的思想是，现在已经获得样本 $x_{1}$, $x_{2}$, ..., $x_{n}$，表明取到这一组样本的概率比较大，将似然函数 $L(\theta)$ 定义为取到这一组变量的概率，那么通过使似然函数最大所得到的 $\hat{\theta}$ 值，就可以作为参数 $\theta$ 的估计值。

$\hat{\theta}(x_{1}$, $x_{2}$, ..., $x_{n})$ 称为参数 $\theta$ 的最大似然估计值，$\hat{\theta}(X_{1}$, $X_{2}$, ..., $X_{n})$ 称为参数 $\theta$ 的最大似然估计量。

对于离散随机变量有：

$$L(\theta) = L(x_{1}, x_{2}, ..., x_{n};\theta) = \prod_{i=1}^{n}p(x_{i};\theta), \theta\in\Theta$$

对于连续变量：

$$L(\theta) = L(x_{1}, x_{2}, ..., x_{n};\theta) = \prod_{i=1}^{n}f(x_{i};\theta), \theta\in\Theta$$

$$L(x_{1}, x_{2}, ..., x_{n};\hat{\theta}) = max_{\theta\in\Theta} L(x_{1}, x_{2}, ..., x_{n};\theta)$$

因为 $L(\theta)$ 和 $\ln{L(\theta)}$ 在同一个地方取到极值，且求对数对于求解较为方便，因此我们采用对数似然方程：

$$\frac{d}{d\theta}\ln{L(\theta)} = 0$$

若包含 $k$ 个待估参数，则：

$$\frac{\partial}{\partial\theta}\ln{L(\theta_{i})} = 0, i = 1, 2, ...,k$$

联立 $k$ 个方程组即可求解待估参数。

# EM 算法
我们先考虑一个例子：
>假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是 $π$, $p$ 和 $q$。进行如下掷硬币实验：先掷硬币A，根据其结果选出硬币B或C，正面选B，反面选硬币C；然后投掷选重中的硬币，出现正面记作1，反面记作0；独立地重复$n$次(n=10)，结果为
>
>$$1011101001$$
>
>我们只能观察投掷硬币的结果，而不知其过程，估计这三个参数。

上例中，观测到的结果为可观测变量，而隐变量为掷硬币A的结果。

对于一个含有隐变量的模型，我们的目标是极大化观测数据，




# 参考资料
1.《概率论与数理统计》第四版 高等教育出版社

2.《统计学习方法》
















