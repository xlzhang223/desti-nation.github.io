---
layout: post
title: EM 算法推导
categories: [Machine Learning]
description: EM 算法
keywords: EM
---

EM 算法是一种无监督的聚类方法，本文对其进行推导。

---

# 概述
EM 算法利用极大似然估计法，对于含有隐变量的模型参数进行迭代估计，每次迭代分为两步：E(expectation)步，求期望；M(maximization)步，求极大值。

# 数学基础
先对EM 算法涉及到的数学基础进行回忆，如果已经掌握可以跳过本部分。

## 极大似然估计
极大似然估计是一种概率论中参数估计方法，常见参数估计方法如下：

![mark](http://pcxhsqn8a.bkt.clouddn.com/blog/181115/Fj40ac0BE3.png?imageslim)

极大似然估计的思想是，现在已经获得样本 $x_{1}$, $x_{2}$, ..., $x_{n}$，表明取到这一组样本的概率比较大，将似然函数 $L(\theta)$ 定义为取到这一组变量的概率，那么通过使似然函数最大所得到的 $\hat{\theta}$ 值，就可以作为参数 $\theta$ 的估计值。

$\hat{\theta}(x_{1}$, $x_{2}$, ..., $x_{n})$ 称为参数 $\theta$ 的最大似然估计值，$\hat{\theta}(X_{1}$, $X_{2}$, ..., $X_{n})$ 称为参数 $\theta$ 的最大似然估计量。

对于离散随机变量有：

$$L(\theta) = L(x_{1}, x_{2}, ..., x_{n};\theta) = \prod_{i=1}^{n}p(x_{i};\theta), \theta\in\Theta$$

对于连续变量：

$$L(\theta) = L(x_{1}, x_{2}, ..., x_{n};\theta) = \prod_{i=1}^{n}f(x_{i};\theta), \theta\in\Theta$$

$$L(x_{1}, x_{2}, ..., x_{n};\hat{\theta}) = max_{\theta\in\Theta} L(x_{1}, x_{2}, ..., x_{n};\theta)$$

因为 $L(\theta)$ 和 $\ln{L(\theta)}$ 在同一个地方取到极值，且求对数对于求解较为方便，因此我们采用对数似然方程：

$$\frac{d}{d\theta}\ln{L(\theta)} = 0$$

若包含 $k$ 个待估参数，则：

$$\frac{\partial}{\partial\theta}\ln{L(\theta_{i})} = 0, i = 1, 2, ...,k$$

联立 $k$ 个方程组即可求解待估参数。

## Jensen 不等式

## 高斯分布/正态分布

## 对数函数常用公式

# EM 算法（公式推导）
我们先考虑一个例子：
>假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是 $π$, $p$ 和 $q$。进行如下掷硬币实验：先掷硬币A，根据其结果选出硬币B或C，正面选B，反面选硬币C；然后投掷选重中的硬币，出现正面记作1，反面记作0；独立地重复$n$次(n=10)，结果为
>
>$$1011101001$$
>
>我们只能观察投掷硬币的结果，而不知其过程，估计这三个参数。

上例中，观测到的结果为可观测变量，而隐变量为掷硬币A的结果。

对于一个含有隐变量的模型，记 $Z$ 为隐变量，$Y$ 为观测变量，我们的目标是要极大化 $Y$ 关于 $\theta$ 的对数似然函数，即极大化：

$$ 
L(\theta) = \log{P(Y;\theta)} = \log{\sum_{Z}P(Y,Z;\theta)} = \log{\left (\sum_{Z}P(Y|Z;\theta)P(Z;\theta)  \right )}
$$

EM 算法通过迭代逐步极大化$L(\theta)$，设在第 $i$ 次迭代之后 $\theta$ 的估计值为 $\theta^{(i)}$, 我们希望新的估计值 $\theta$ 能够使 $L(\theta)$ 增加，即：

$$
L(\theta) - L(\theta^{(i)}) = \log{\sum_{Z}P(Y,Z;\theta)} - \log{\sum_{Z}P(Y,Z;\theta^{(i)})}  \\

= \log{\left (\sum_{Z}P(Y|Z;\theta)P(Z;\theta)  \right )} - \log{\sum_{Z}P(Y,Z;\theta^{(i)})}
$$

对第一项的分子分母同时加入 $P(Z &#124 Y;\theta^{(i)})$ ，此时可以利用 Jensen 不等式求其下界：

(注：$\sum_{Z}P(Z &#124 Y;\theta^{(i)})=1$)


$$
L(\theta) - L(\theta^{(i)}) = \log{\left (\sum_{Z}  P(Z|Y;\theta^{(i)})\frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}  \right )} - \log{\sum_{Z}P(Y,Z;\theta^{(i)})} \\

\geqslant \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ \frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)}))}} - \log{\sum_{Z}P(Y,Z;\theta^{(i)})} \\

= \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ \frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}} - \log{P(Y;\theta^{(i)})} \\

= \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ \frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}} - \sum_{Z}  P(Y,Z;\theta^{(i)}) \log{P(Y;\theta^{(i)})} \\

= \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ \frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)}))P(Y;\theta^{(i)})}}
$$

因此，$L(\theta)下界为：$

$$
L(\theta) \geqslant L(\theta^{(i)}) + \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ \frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})}} = B(\theta, \theta^{(i)})
$$

那么任何可以使 $B(\theta, \theta^{(i)})$ 增长的 $\theta$，都可以使$L(\theta)$增长，我们选择 $\theta^{(i + 1)}$ 使 $B(\theta, \theta^{(i)})$ 达到极大，我们省略对于极大化而言时常数的项，

$$
\theta^{(i + 1)} = \arg \max_{\theta}B(\theta, \theta^{(i)}) \\
= \arg \max_{\theta}\left ( L(\theta^{(i)}) + \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ \frac{P(Y|Z;\theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})}} \right ) \\

= \arg \max_{\theta}\left (\sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ (P(Y|Z;\theta)P(Z;\theta))} \right ) \\

= \arg \max_{\theta}\left (\sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ (P(Y,Z;\theta))} \right ) \\

= \arg \max_{\theta}Q(\theta, \theta^{(i)})
$$

即最终表示为求$Q$函数以及其极大化。EM 算法通过不断求解下界的极大化来逼近对数似然函数的极大化。

# EM 算法
输入：观测变量数据 $Y$，隐变量数据 $Z$，联合分布 $P(Y, Z;\theta)$，条件分布 $P(Z | Y;\theta)$；

输出：模型参数 $\theta$。

步骤：

1. 选择参数的初始值 $\theta^{(0)}$，开始迭代；
2. repeat (直到收敛)
    {

    E 步：记 $\theta^{(i)}$ 为第 $i$ 次迭代参数 $\theta$ 的初始值，在第 $i + 1$ 步的时候计算：
    $$
    Q(\theta, \theta_{(i)}) = \sum_{Z}  P(Z|Y;\theta^{(i)}) \log{ (P(Y,Z;\theta))}
    $$
    M 步：求使 $Q$ 极大化的 $\theta$
    $$
    \theta^{(i+1)}= \arg \max_{\theta}Q(\theta, \theta^{(i)})
    $$
    }

# EM 算法应用于高斯混合模型的求解

## 高斯混合模型

高斯混合模型指具有如下概率分布的模型，即 $k$ 个参数为 $\theta_{k}$ 的分布密度为 $\phi$ 高斯模型以权重 $\alpha_{k}$ 组合在一起的模型，公式表示为：
$$
P(y | \theta) = \sum_{k=1}^{K} \alpha_{k} \phi(y | \theta_{k})
$$
其中，$\alpha_{k}$ 是系数，$\alpha_{k} \geqslant 0$， $\sum_{k=1}^{K} \alpha_{k}$；$\phi(y | \theta_{k})$ 是高斯分布密度，$\theta_{k} = (\mu_{k}, \sigma_{k}^{2})$，

$$
\phi(y | \theta_{k}) = \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y-\mu_{k})^2}{2\sigma_{k}^{2}} \right)
$$
称为第 $k$ 个分模型。

## 用 EM 算法求解高斯混合模型（公式推导）

### 准备工作：明确隐变量，写出 $\log{ (P(Y,Z;\theta))}$
观测数据的产生：假设观测数据为 $y_{i}$，$j = 1, 2, ..., N$，是这样产生的：先依照概率 $\alpha_{k}$ 选择第k个分模型，再由第k个分模型产生观测数据。观测数据来自于哪一个分模型是观测不到的，因此把该变量设置为隐变量，记 $\gamma_{jk}$，第j个观测来自于第k个模型该值为1，反之为0。

先求完全数据 $y, \gamma$ 的似然函数：
$$
P(y, \gamma ; \theta) = \prod_{j = 1}^{N}P(y_{j}, \gamma_{j1}, \gamma_{j2}, ..., \gamma_{jK};\theta)\\
= \prod_{j = 1}^{N} \prod_{k = 1}^{K} [\alpha_{k} \phi(y_{j} ; \theta_{k})]^{\gamma_{jk}}
$$
上式比较难理解，主要注意连乘符号可以交换，另外上式表示完全数据的似然函数等于N个观测数据的似然函数之积，等于每个观测所来自于的子模型的概率分布的积。好了，我们继续推导。

$$
P(y, \gamma ; \theta) = \prod_{k = 1}^{K} \alpha_{k}^{n_{k}} \prod_{j = 1}^{N} [\phi(y_{j} ; \theta_{k})]^{\gamma_{jk}} \\
= \prod_{k = 1}^{K} \alpha_{k}^{n_{k}} \prod_{j = 1}^{N} \left[   \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}} \right)      \right]^{\gamma_{jk}}
$$

其中，$n_{k} = \sum_{j=1}^{n}\gamma_{jk}$。

有了完全数据的似然函数，我们对其求自然对数：

$$
\ln{P(y, \gamma ; \theta)} = \ln{\left( \prod_{k = 1}^{K} \alpha_{k}^{n_{k}} \prod_{j = 1}^{N} \left[   \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}} \right)      \right]^{\gamma_{jk}} \right)}\\

= \sum_{k = 1}^{K}  
\left\{
 \ln{\alpha_{k}^{n_{k}}} + \ln{ \left( \prod_{j = 1}^{N}  \left[   \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}} \right)  \right]^{\gamma_{jk}} \right)}
 \right\}
\\

= \sum_{k = 1}^{K}  
\left\{
 n_{k}ln{\alpha_{k}} + \sum_{j = 1}^{N} \ln{ \left(      \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}} \right)  ^{\gamma_{jk}} \right)}
 \right\}
\\

= \sum_{k = 1}^{K}  
\left\{
 n_{k}ln{\alpha_{k}} + \sum_{j = 1}^{N} \gamma_{jk} \ln{ \left(      \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}} \right)   \right)}
 \right\}
\\

= \sum_{k = 1}^{K}  
\left\{
 n_{k}ln{\alpha_{k}} + \sum_{j = 1}^{N} \gamma_{jk} \ln{ \left(      \frac{1}{\sqrt{2\pi}\sigma_{k}}exp\left( -\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}} \right)   \right)}
 \right\}
\\

= \sum_{k = 1}^{K}  
\left\{
 n_{k}ln{\alpha_{k}} + \sum_{j = 1}^{N} \gamma_{jk}  \left(     \ln{\frac{1}{\sqrt{2\pi}}} - \ln{\sigma_{k}-\frac{(y_{j}-\mu_{k})^2}{2\sigma_{k}^{2}}} \right)
 \right\}
\\

$$

### E 步：求 $Q$ 函数











# 参考资料
1.《概率论与数理统计（第四版）》 浙江大学 高等教育出版社 

2.《统计学习方法》李航著
















