---
layout: post
title: 神经网络中的反向传播
categories: [Machine Learning]
description: 神经网络中的反向传播
keywords: 反向传播
---

反向传播推导

---
## 1. 前言

首先要明确`反向传播`（Backpropagation）是计算梯度的方法，训练模型用的是优化算法（如SGD，Adam，Adagrad，RMSprop），而大部分优化算法是需要计算梯度的，所以会用到反向传播算法，所以反向传播是深度学习优化算法的基石。换句话说，反向传播用来计算梯度，优化算法用来决定参数如何根据梯度来更新。

## 2.数学基础：链式法则求导 

![](https://github.com/desti-nation/desti-nation.github.io/raw/master/images/posts/bp/chainrule.jpg)

## 3.正向传播的计算图 
我们举一个例子进行推导，下图是一个两层的感知机的正向传播的计算图：
![](https://github.com/desti-nation/desti-nation.github.io/raw/master/images/posts/bp/graph.jpg)
其中网络输入为$x$，输出为$o$， $\phi$为激活函数，损失$J$由$L = l(o,y)$和正则化项$s$组成, 

$$s = \frac{λ}{2} ({∥W^{(1)}∥}^{2}_{F}+{∥W^{(2)}∥}^{2}_{F})$$。



## 4. 反向传播





## 4. 参考文献

1. [ML Lecture 7: Backpropagation [video]](https://www.youtube.com/watch?v=ibJpTrp5mcE)
2. [ML Lecture 7: Backpropagation [ppt]](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/BP.pdf)
3. [动手学深度学习：正向传播、反向传播和计算图](http://zh.gluon.ai/chapter_deep-learning-basics/backprop.html)







