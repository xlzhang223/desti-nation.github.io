---
layout: post
title: 神经网络中的反向传播
categories: [Machine Learning]
description: 神经网络中的反向传播
keywords: 反向传播
---

反向传播推导

---
## 1. 前言

首先要明确`反向传播`（Backpropagation）是计算梯度的方法，训练模型用的是优化算法（如SGD，Adam，Adagrad，RMSprop），而大部分优化算法是需要计算梯度的，所以会用到反向传播算法，所以反向传播是深度学习优化算法的基石。换句话说，反向传播用来计算梯度，优化算法用来决定参数如何根据梯度来更新。

## 2.数学基础：链式法则求导 

![](https://github.com/desti-nation/desti-nation.github.io/raw/master/images/posts/bp/chainrule.jpg)

## 3.正向传播的计算图 
我们举一个例子进行推导，下图是一个两层的感知机的正向传播的计算图：
![](https://github.com/desti-nation/desti-nation.github.io/raw/master/images/posts/bp/graph.jpg)
其中网络输入为$x​$，输出为$o​$， $\phi​$为激活函数，损失$J​$由$L = l(o,y)​$和正则化项$s​$组成, 

$$
s = \frac{λ}{2} ({∥W^{(1)}∥}^{2}{F}+{∥W^{(2)}∥}^{2}{F})
$$


## 4. 反向传播

![](https://github.com/desti-nation/desti-nation.github.io/raw/master/images/posts/bp/form.jpg)



## 5. 梯度消失/爆炸

> 本质原因：梯度反向传播过程中的连乘效应

误差从输出层反向传播的时候，在每一层都要乘激活函数 $f(x)$ 的导数，常见的 sigmoid 和 tanh 激活函数记为 $\sigma(x)$ 和 $tanh(x)$，其导数为：


$$
\sigma^{’}(x) = \sigma(x)(1 - \sigma(x))  \in [0, 0.25]\\
\tanh^{’}(x) = 1 - (\tanh(x) )^2 \in[0, 1] \\
$$


其值域都小于1，因此误差经过每一层的时候会不断递减。当网络层数比较深的时候，梯度不断递减甚至可能消失，这使整个网络很难训练。这被称为梯度消失。

同理，梯度爆炸发生在网络的激活函数的导数大于1的时候。

如何避免两类情况的出现：

1. 使用浅层网络
2. 使用 ReLU 函数 或者 Leaky ReLU
3. LSTM 的结构设计可以改善 RNN 中的梯度消失问题

## 6. 代码实现



## 7. 参考文献

1. [ML Lecture 7: Backpropagation [video]](https://www.youtube.com/watch?v=ibJpTrp5mcE)
2. [ML Lecture 7: Backpropagation [ppt]](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/BP.pdf)
3. [动手学深度学习：正向传播、反向传播和计算图](https://zh.diveintodeeplearning.org/chapter_deep-learning-basics/backprop.html)
4. [用好维度分析，不要直接求导](https://zhuanlan.zhihu.com/p/25202034)
5. [活学活用：实现一个简单的神经网络](https://zhuanlan.zhihu.com/p/31708783)







